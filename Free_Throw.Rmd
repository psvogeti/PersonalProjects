---
title: "Predicting the Free Throw"
author: "Pranav Vogeti"
date: "2023-11-22"
output: 
  html_document:
    mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{css, echo=FALSE}
body {
  background-color: papayawhip
}
```

```{r message = FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(lubridate)
library(kableExtra)
```

# Introduction

As the new 2023-2024 NBA season kicks into gear, and as I am beginning to learn more about predictive modeling, I was interested in seeing if there was a way to model the NBA free throw within the context the shot is performed. That context includes but is not limited to, the timestamp at which the free throw was taken, the player that took the free throw, and whether the score of the game has any impact on the shot.

### The Focus Question

How might the NBA free throw shot be modeled such that the model can provide better insight then the player's career average free throw?

### Motivation

When watching an NBA game, the viewer is exposed to many types of shots, each equipped with their own play to achieve that particular shot. One such shot is the famed dunk. Another is a layup. Additional shots include the mid-range, the three-point shot, and the focus of this report: the free-throw.

The free throw is perhaps one of the most interesting shots in the game. The most obvious indicator is that it is by convention, *free*. When a free throw shot is taken, any and all play stops. The play clock pauses, fans become muted or monsters depending on what team is taking the shot, and if taken at the right moments, can prove to be pivotal indicators on a team's chances of winning in close games.

What spurs this investigation? The fact is, it is a **KNOWN** shot. Everybody, regardless of what age you are, how many years you have played, how skilled or talented you are, takes the exact same shot at the exact same point on the line. It is the only shot in the game that is known by every player. However, basing the free throw off of its known attributes ignores the true complexity behind the shot. It is a rather simple shot to learn and prepare for. The difficulty lies in the context at which it is taken.

For instance, a free throw in the opening minutes of a game, while the final score may disagree, will have significantly less impact morale wise than a free throw taken in the closing moments, when that shot determines the outcome of the close game. The pressure difference is hardly ignorable.

Moreover, the player taking the shot must be considered as well. Consider the world-famous basketball player [Shaquille O' Neal](https://www.basketball-reference.com/players/o/onealsh01.html). He was a monster in paint. Over his career, he has averaged 23.7 points per game, 10.9 rebounds per game, all on 58.2% overall field goal percentage. But his free throw percentage was a meager 52.7%, nearly a coin flip, and in some of those seasons, he performed worse than a coin flip. ]. Because of this, teams began implementing the "Hack - A - Shaq" strategy in the closing minutes of the game because the statistics were in the opposing team's favor. At best, Shaq's free throw performance was random. So how might the player impact the shot as well?

# Background

## Data Description

All of the data we will use will be at the [References] portion of this report.

### About the Original Data

#### About the Play-By-Play Data

The author that has provided the data for us to use is Vladislav Shufinsky. He has compiled NBA play-by-play data from 1996-2022 and made it open-source on Kaggle to allow us to use it for excursions like this[^1].

[^1]: <https://www.kaggle.com/datasets/brains14482/nba-playbyplay-and-shotdetails-data-19962021/data>

The data set we will use will contain a whole season's log of play-by-play data from the 2021-2022 NBA season. While the entire dataset consists of four subdata sets, we will choose to use the data originally sourced from *data.nba.com* This means that we will use the file called "*datanba_2022.csv*"

#### About the Player Shot Statistics Data

The original source of this data comes from Sports Reference, namely Basketball Reference[^2]. The data was queried and appended through Excel's Power Query feature and made use of Basketball Reference's tables for each team in the 2021-2022 season. The *Advanced* regular season table was used to obtain player data on games played, minutes played, usage rate, and free throw rate. The *Per Game* regular season table was used to obtain player data on free throws, field goal percentage, and effective field goal percentage. The queries have the format [*https://www.basketball-reference.com/teams/{team_abbr}/2022.html#per_game*](https://www.basketball-reference.com/teams/%7Bteam_abbr%7D/2022.html#per_game){.uri} and [*https://www.basketball-reference.com/teams/{team_abbr}/2022.html#advanced*](https://www.basketball-reference.com/teams/%7Bteam_abbr%7D/2022.html#advanced){.uri} for each team with Basketball Reference's team abbreviation standard.

[^2]: <https://www.basketball-reference.com/teams/MIN/2022.html>

The data hyperlinked is a sample one for Minnesota, but any team can be easily navigated to from the hyperlinked URL.

### The Key Variables

-   The key variables from the *Play-By-Play* we will use for this excursion:
    -   `cl` the time stamp at which the significant play occurred
    -   `de` the short form description of what the play was
    -   `hs` the home team total points at the `cl` moment
    -   `vs` the visiting team total points at the `cl` moment
    -   `pts` the pts scored by the respective `hs` or `vs` at the `cl` moment
    -   `PERIOD` the quarter of the game so as to distinct `cl`
    -   `GAME_ID` the numerical identifier of the game being played
-   The key variables from the *Player Shot Statistics* data set we will use for this excursion:
    -   `player` the NBA player of focus
    -   `team` the team of the NBA `player`
    -   `gms_played` the total number of games the `player` has participated in for the 2021-2022 regular season
    -   `min_played` the total number of minutes the `player` has participated in for the 2021-2022 regular season
    -   `usage_pct` an estimate of the percentage of `team` plays used by a `player` while he was on the floor [^3]
    -   `fgm` the average number of made field goals per game by the basketball `player`
    -   `fga` the number of field goal attempts per game by the basketball `player`
    -   `fg_pct` the ratio between `fgm` and `fga`
    -   `eff_fg_pct` "effective field goal percentage; the formula is (FG + 0.5 \* 3P) / FGA. This statistic adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal"[^4]
    -   `ftm` the average number of made field goals per game by the basketball `player`
    -   `fta` the number of field goal attempts per game by the basketball `player`
    -   `ft_pct` the ratio between `ftm` and `fta`
    -   `ft_rate`the ratio between `fta` and `fga` and can be viewed as the rate at which the `player` visits the free throw line based on shots taken [^5]

[^3]: <https://www.basketball-reference.com/about/glossary.html>

[^4]: <https://www.basketball-reference.com/about/glossary.html>

[^5]: <https://www.nbastuffer.com/analytics101/free-throw-rate/>

### Assumptions and Unusuality

One assumption that will underline this report is the fact the assumption that any and all free throws, for the same stage of the game $S_n$ is the same. Consider a shooting regular foul $L$ on a two-point shot that occurred at the very beginning of the first quarter ($S_n = 1.1$). Similarly, consider the exact same foul for the last minutes of the fourth quarter ($S_n = 4.4$). Foul shot $F$ has a success probability $p = P(F)$ Across both cases $S_n$ is different so $P(F)$ may be different. But if $L$ occurs under the same $S_n$, $P(F)$ is the same. The only thing differentiating the probability of $F$ is $S_n$.

Moreover, while in $L$, $F$ is independent and identical. This gives us the following:

Given foul $L$ has free throw $F$ occuring at some interval in the game $S_n$:

$$
s \in S_n \mid S_n = \{1.1, 1.2, 1.3, 1.4, 1.5, 2.1, ..., 2.5, 3.1, ..., 6.5\}
$$

$$
\forall F \in s
$$

We Assume That:

$$
F \sim \text{Bernoulli(p)}
$$

Similarly, we will do the same for game point differential $D$. $D$ is categorical with five labels:

1)  `HOME DOMINATION` $(H_D)$ if home team is up by at least +15 or more

2)  `HOME LEADING`$(H_L)$ if home team is up as at least +5 but less than +15

3)  `TIGHT GAME`$(T_G)$ if home team is as at least -5 but less than +5 within the visiting team

4)  `VISIT LEADING` $(V_L)$ if home team is down at least -15 but less than -5

5)  `VISIT DOMINATION`$(V_D)$ if home team is down at least -15 or less

We use the $S_n$ assumption above to make this assumption. The assumption is that for any and all free throws, for the categorically same $D$ occurring in $S_n$ , foul shot $F$ with probability $P(F)$ is the same.

Consider a shooting regular foul $L$ on a two-point shot that occurred at the very beginning of the first quarter ($S_n = 1.1$) with the home team up by 10 ($D = H_L$). Similarly, consider the exact same foul just moments later ($S_n=1.1$) but now the visiting team is up by 5 ($D = V_L$). Foul shot $F \in L$ has a success probability $p = P(F)$. Across both cases $D$ is different so $P(F)$ may be different. But if $L$ occurs under the same $D$, $P(F)$ is the same. The only thing differentiating $P(F)$ is $D$.

Moreover, while in $L$, $F$ is independent and identical. This gives us the following:

Given foul $L$ has free throw $F$ occurring at some interval in the game $S_n$ with a current game point differential $D$ :

$$
d \in D \mid D = \{H_D, H_L, T_G, V_L, V_D\}
$$

$$
\forall F \in d
$$

We Assume That:

$$
F \sim \text{Bernoulli(p)}
$$

### Methodology

As stated before the goal is to create some model $m$, specific to the the state of the game $S_n$ and the score of the game $D$, that can accurately model $P(F)$

As a base line reference, we let the base line model be the null model $n$ with only $\beta_0$ representing the $p$'s season average free throw such that: $$
n \sim \{Y = \beta_0 \mid \beta_0 \in [0, 1]\}$$

Where $Y$ value represents $P(F)$

Once we find $m$, we will then assign $m$ to its respective player $p$ to become $m_p$. We will do this for all $p$ so that we can accomplish a dynamic function $f$ that, when passed in $p$, $S_n$ and $D$, returns the appropriate $m_p$

How do we get $f$?

Recall that $m$ depends on three things: $p$ which we choose, and then all instances of $S_n$, and $D$. Once we choose $p$, we get $m_p$. $S_n$ and $D$ then become the categorical specifications alongside $m_p$ that $m$ is built on. The numerical predictors of $m_p$ depends on $p$'s statline. However, it is also highly likely that $m$ may not be built if $p$ does not have any $F$ in a particular $s \in S_n$ or a particular $d \in D$. If this is the case, then we avoid building $m$ and we will use $n$ to replace $m$ as the deferring model, where $n$ is only $p$'s season average free throw percentage

Here is the visualization of this how $p$'s models may vary depending on $D$ (the rows) and $S_n$ (the columns). Note, $n$ may be at any index of the array, it is just demonstrated that $n$ exists when a instance of $S_n$ or $D$ is not readily available

```{=tex}
\begin{array}{c|ccccccc} 

& 1.1 & 1.2 & 1.3 & 1.4 & 1.5 & \cdots & 6.5 \\


H_D & m_{1.1\times H_D} & m_{1.2\times H_D} & m_{1.3\times H_D} & m_{1.4\times H_D} & m_{1.5\times H_D} & \cdots & m_{6.5\times H_D}\\ 

H_L & m_{1.1\times H_L} & \ddots & m_{1.3\times H_L} & \cdots & \cdots & \cdots & \vdots\\ 

T_G & m_{1.1\times T_G} & n & \ddots & m_{1.4\times T_G} & n & \cdots & \vdots\\ 

V_L & m_{1.1\times V_L} & \cdots & \cdots & \ddots & m_{1.5\times V_L} & \cdots & \vdots\\

V_D & m_{1.1\times V_D} & \cdots & n & \cdots & \cdots & \cdots & m_{6.5\times V_D}\\
\end{array}
```
### Report Intentions

The rest of this report will showcase the data we are working with and their cleaned sets. Then, our analyses gets divided into three parts: part one will focus on finding $m_p$ for some $p$ and part two will aim to build $m$. Part three aims to build $f$ and show how $f$ can be used in as many informational contexts as possible. We will discuss about $f$ and highlight its pros and cons in our [Discussion] section. Finally, we will also provide alternative approaches to our query and also provide possible extensions as well.

# Analysis

### Data Cleanup

Here we will clean up our mentioned data sets and add and remove certain variables as needed. Any significant changes are reported at the bottom of the clean up section.

```{r read-print-raw-data, echo=TRUE}

raw_pbp_2022_data <- read.csv("../data/datanba_2022.csv")

##print the first 15 rows of this raw data
head(raw_pbp_2022_data, n=15)


##the data that has each player's relevant shooting stats
raw_player_data <- read.csv("../data/power_query_ft_model.csv")
```

Here we clean up our raw play-by-play data and output the cleaned data that we will work with.

```{r clean-pbp-data, echo=TRUE}

##the last name of the player, since that is what is in the pbp data
player_regex = "\\]\\s\\w+"
##to capture all "free throws"
free_throw_literal_regex = "Free Throw"
##to capture the score as a numeric from the description
score_regex = "[0-9]+"
##to capture the team code
team_regex = "\\[[A-Z]{3}"



pbp_2022_data <- raw_pbp_2022_data %>% select(
  cl, de, hs, vs, pts, PERIOD, GAME_ID
)

##get the home and away scores
free_throws_2022 <- pbp_2022_data %>% 
  filter(str_detect(de, free_throw_literal_regex)) %>%
  group_by(GAME_ID) %>%
  ## responsible for getting the teams involved in this game
  mutate(
    team = str_extract(de, regex(team_regex, FALSE)),
    player_team = substring(team, 2, 5), 
  ) %>%
  ##responsible for getting the player shooting the free throw
  mutate(
    player_identifier = str_extract(de, regex(player_regex, TRUE)), 
    player_shooting = substring(player_identifier, 3, last = 1000000L)
  ) %>% 
##responsible for classifying certain stage of the game in period.int format
  mutate(
    interval = case_when(
      cl <= "01:00" ~ 0.5,
      cl <= "03:00" ~ 0.4, 
      cl <= "06:00" ~ 0.3, 
      cl <= "09:00" ~ 0.2,
      cl <= "12:00" ~ 0.1), 
    stage_of_game = PERIOD + interval 
  ) %>% 
  ##difference in scores, postive is home team is ahead
  mutate(
    pt_difference = hs - vs, 
    pt_difference_cat = case_when(
      pt_difference >= 15 ~ "HOME DOMINATION", #[15, inf)
      pt_difference <= -15 ~ "VISIT DOMINATION", #(-inf, -15]
      pt_difference < -5 ~ "VISIT LEADING", #(-15, -5]
      pt_difference < 5 ~ "TIGHT GAME", #[-5, 5)
      pt_difference < 15  ~ "HOME LEADING") #[5, -15)
  ) %>%
  ##QoL change
  rename(gm_quarter = PERIOD) %>%
  select(GAME_ID, stage_of_game, gm_quarter, de, hs, vs, player_shooting, player_team, pts, pt_difference, pt_difference_cat)
  
##print out the first 10 rows of this new data set
head(free_throws_2022, n = 10)
```

As seen above, few new variables were introduced in this cleaner set:

-   `gm_quarter` the current NBA defined quarter of this match identified by the `GAME_ID`; any value above 4 is considered to be the overtime quarter

-   `player_shooting` the player shooting the free throw

-   `player_team` the team of the player shooting the free throw

-   `pt_difference` the difference between `hs` and `vs`, positive indicates home team score advantage

-   `pt_difference_cat` characterizes `pt_difference` into five distinct categories

    1)  `HOME DOMINATION` if `pt_difference` as at least +15 or more
    2)  `HOME LEADING` if `pt_difference` as at least +5 but less than +15
    3)  `TIGHT GAME` if `pt_difference` as at least -5 but less than +5
    4)  `VISIT LEADING` if `pt_difference` as at least -15 but less than -5
    5)  `VISIT DOMINATION`if `pt_difference` as at least -15 or less

Here we clean up our raw shot statistic data and output the cleaned data that we will work with.

```{r clean-player-data, echo=TRUE}
clean_player_data <- raw_player_data %>%
  mutate(min_per_gm = min_played/gms_played) %>%
  # remove players without any shot activity, especially free throws
  filter(fta != 0) %>%
  filter(fga != 0) %>%
  rename(player_team = team) %>%
  select(player_team, player, gms_played, min_per_gm, usage_pct, fgm, fga, fg_pct, eff_fg_pct, ftm, fta, ft_pct, ft_rate)

#print the first 10 rows of this data set
head(clean_player_data, n = 10)
```

As seen above, one new variable was introduced in this cleaner set:

-   `min_per_gm` the `player` ratio between `gms_played` and `min_played`

Finally, we join both of these data sets together to be used for our analysis.

```{r join-data, echo=TRUE}
## get the last name of the player in our shot statistic data
clean_player_data <- clean_player_data %>% mutate(last_name = sub(".*\\s", "", player))

## join the shot statistic data to the free throw data by matching the player team and the player shooting (via our last name column)
ft_complete <- free_throws_2022 %>%
  inner_join(clean_player_data, by = c("player_team", "player_shooting" = "last_name"))

## print the first 10 rows of this data set
head(ft_complete, n = 10)
```

#### Analysis: Finding $m_p$

Now comes the fun part, building $m_p$! Since we are predicing on a binary result, logistic regression makes sense here.

Here we define $m_p$ to be the shooting stats of $p$ as predictors of $F$. We will factor in $S_n$ and $D$ in [Analysis: Finding $m$]

The goal is to let $m_p \sim \sigma(m_\text{lin})$ where $m_\text{lin}$ is the linear model:

$$
Y = \beta_0 + \beta_\text{min_per_gm}X_\text{min_per_gm} + \\  \beta_\text{usage_pct}X_\text{usage_pct} + \\
\beta_\text{fgm}X_\text{fgm} + \beta_\text{fga}X_\text{fga} +  \beta_\text{fg_pct}X_\text{fg_pct} \beta_\text{eff_fg_pct}X_\text{eff_fg_pct} + \\
\beta_\text{ftm}X_\text{ftm} + \beta_\text{fta}X_\text{fta} + \beta_\text{ft_pct}X_\text{ft_pct} + \beta_\text{ft_rate}X_\text{ft_rate}
$$

```{r m-p, echo=TRUE}
##create the log-reg model on p-statline
ft_model <- glm(pts ~ min_per_gm + usage_pct + fgm + fga + fg_pct + eff_fg_pct + ftm + fta + ft_pct +ft_rate,
                data = ft_complete, family = "binomial") 

##view the summary
summary(ft_model)
```

Based on our summary, it seems we can comfortably reduce the complexity of $m_p$. Since we are operating under $\alpha = 0.05$, we choose to keep:

$$
\beta_\text{min_per_gm},\\
\beta_\text{fg_pct}, \beta_\text{eff_fg_pct},\\
\beta_\text{ftm}, \beta_\text{fta},\\ \beta_\text{ft_pct},\beta_\text{ft_rate} 
$$

```{r ft-model-2, echo=TRUE}
##create the log-reg model on p-statline
ft_model_updated <- glm(pts ~ min_per_gm + fg_pct + eff_fg_pct + ftm + fta + ft_pct +ft_rate, data = ft_complete, family = "binomial") 

##view the summary
summary(ft_model_updated)
```

When running $m_p$ summary $\beta_\text{ft_rate}$ obtained an even stronger significance level under $\alpha = 0.05$.

Now, we begin training and testing our data. The method we use is not ours, rather what we found from a quick YouTube query[^6] in order to easily split our complete data in a 80% train set and 20% test set. We will divide our rows of data by two indices, each with a weighted probability (0.8 and 0.2) of occurring. This gives us random sets to work with. We will then train $m_p$ on the training data and test it on the test data.

[^6]: <https://www.youtube.com/watch?v=Irlfb8ELzis&ab_channel=RidvanGedik>

The splitting method we use can be found [here](https://www.youtube.com/watch?v=Irlfb8ELzis&ab_channel=RidvanGedik)

```{r train-test-split, echo=TRUE}
##split data into training (80%) and testing (20%)
TRAIN_SPLIT = 0.8
TEST_SPLIT = 0.2
DATA_SIZE = nrow(ft_complete)

##reproduceable split
set.seed(715)
##choose between indexes 1, 2 with one index at 80% chance and the other at 20%
index_choice <- sample(2, DATA_SIZE, replace = TRUE, prob=c(TRAIN_SPLIT, TEST_SPLIT))
train_data <- ft_complete[index_choice==1, ]
test_data <- ft_complete[index_choice==2, ]

##train our model
ft_model_train <- glm(pts ~ min_per_gm + fg_pct + eff_fg_pct + ftm + fta + ft_pct +ft_rate, data = train_data, family = "binomial")

##test our trained model
ft_model_test <- predict.glm(ft_model_train, test_data, type='response')

##print out the results of our tested predictions
head(ft_model_test, n = 10)

##add each model predicted free throw to the corresponding player
NUM_PLAYERS = nrow(test_data)
test_data_predictions <- test_data %>% mutate(model_prob = 0)
for (i in 1:NUM_PLAYERS) {
  test_data_predictions$model_prob[i] = ft_model_test[[i]]
}

test_data_predictions <- test_data_predictions %>% ungroup() %>%
  select(stage_of_game, player_shooting, pts, min_per_gm, fg_pct, eff_fg_pct, ftm, fta, ft_pct, model_prob)

##print the first few lines of our predicted data
head(test_data_predictions, n = 10)
```

Again the above output is decent because our model probabilities in `model_prob` are very close to `ft_pct`. But before we evaluate `model_prob` accuracy, we still have not done anything with $S_n$ and $D$ yet!

This concludes part 1 of building a holistic $m_p$. In this sub-section, we have built $m_p$ using only the statline of $p$ to give an adjusted free throw probability $P(F)$. In the next section, we will adjust $P(F)$ given by $m_p$ by factoring in $S_n$ and $D$ to create $m$.

#### Analysis: Finding $m$

At this point, we have accounted for our numerical predictors. What we need to do is provide our model context with our categorical variables $S_n$ and $D$ and see how that may impact $P(F)$

Before we jump in, let's use the context of one particular player $p = \text{LeBron James}$. Why him? LeBron has been in the conversation for the greatest player of all time in the NBA. He has been mentioned in countless in-game situations and hypotheticals, making him a robust choice to see what we need to consider when factoring in $S_n$ and $D$

Before we adjust $m_p$ as well, we need to join LeBron's statline to his free throw data. This will allow us to graph off of one dataset.

```{r LeBron, echo = TRUE}
player_of_interest = "LeBron James"
split_names <- strsplit(player_of_interest, " ")
last_name = tail(split_names[[1]], 1)

##get the team based on our player
team_of_interest = clean_player_data %>%
  filter(player == player_of_interest) %>% pull(player_team) %>% str_extract(regex("\\w+"))

##get the player's free throw chart
free_throws_lebron <- free_throws_2022 %>%
  filter(player_team == team_of_interest) %>%
  filter(player_shooting == last_name)

lebron_shooting_data <- clean_player_data %>% filter(player == player_of_interest)

model_data_lebron <- full_join(free_throws_lebron, lebron_shooting_data, by = "player_team")
```

Now, let's see how his $F$ depends on $S_n$ and $D$.

```{r F-Sn, echo=TRUE}
ggplot(data=free_throws_lebron, aes(x=stage_of_game, y=pts, color = pt_difference_cat)) + 
  geom_jitter(alpha=.5, height=0.05) +
  facet_wrap(vars(gm_quarter), scales = "free_x") + 
  theme_dark() +
  theme(plot.background = element_rect(fill = "papayawhip")) + 
  theme(legend.background = element_rect(fill = "papayawhip")) + 
  scale_colour_manual(
    values = c("papayawhip", "burlywood1", "darkgoldenrod1", 
               "cyan1", "cyan4")) +
  labs(color = "Point Differential") +
  xlab("Stage of the Game") +
  ylab("Free Throw Shot Result") +
  ggtitle("LeBron's Free Throw Shot Results By Quarter and Stage of Game", 
          subtitle = "2021-2022 NBA Regular Season")
```

There is a lot going on in this faceted collection of graphs.

We notice that the first quarter facet shows a lot of $F$ under $T_G$. This is expected because we defined it to be $T_G = [-5, 5)$ which is highly likely to be the result in the first quarter. This similar observation can be seen in "quarter 5" (overtime) facet, where $T_G$ makes its blanket appearance again. This is also expected behavior because in overtime, the teams have already played four quarters of basketball to have a tie game, and so the competition will be back and forth in overtime. Analyzing the fourth quarter facet, we see a greater density of missed $F$ than in any other facet. This is what we had a hunch on early on in this report. However, across all facets, in seems that $T_G$ dominates in the missed $F$ collection.

The main takeaway with regards to this report is this: it is worth our time to use our categorical predictors $S_n$ and $D$ when dealing with $f$. Doing so, will help us model situations we saw in facets 1, 4, and 5.

First we link the probabilities outputted by $m_p$ to our data set.

```{r adding-p, echo=TRUE}
##first, let's add all the P from our model 
## make predictions on all of the data
ft_model_probs <- predict.glm(ft_model_train, ft_complete, type='response')

##add each probability to the corresponding player
NUM_PLAYERS = nrow(ft_complete)
ft_complete <- ft_complete %>%
  mutate(model_prob = 0)
for(i in 1:NUM_PLAYERS) {
  ft_complete$model_prob[i] = ft_model_probs[[i]]
}
```

Now, we can use `model_prob` as a predictor when dividing our data along `stage_of_game` and `pt_difference_cat`.

```{r ft-model-full-setup, echo=TRUE}
##convert to factors so that S_n and D are treated categorically
ft_complete$stage_of_game <- as.factor(ft_complete$stage_of_game)
ft_complete$pt_difference_cat <- as.factor(ft_complete$pt_difference_cat)

##create the log-reg model on p-statline
ft_model.cats <- glm(pts ~ stage_of_game + pt_difference_cat + model_prob, data = ft_complete, family = "binomial") 

##view the summary
summary(ft_model.cats)
```

Well this is slightly awkward! After spending so much time talking about the potential impacts of $S_n$ and $D$, we only found so many significant $s \in S_n$ and no significant $D$! What is interesting to see by the summary output is our significant $s_n$ are (under $\alpha = 0.05$):

For 
$$
s_n \in S_n \mid S_n = \{1.1, 1.2, 1.3, 1.4, 1.5, 2.1, ..., 2.5, 3.1, ..., 6.5\}
$$

We see 
$$
s_n = \{1.2, 1.3, 2.4, 3.1, 3.2, 3.3, 3.4, 3.5, 4.1, 4.2\}
$$

What is even more interesting is the positive effect that our model observes.

> This model implies that for a free throw taken in $s = 1.2$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.15$ when compared to other contexts.

> This model implies that for a free throw taken in $s = 1.3$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.18$ when compared to other contexts.

> This model implies that for a free throw taken in $s = 2.4$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.18$ when compared to other contexts.

> This model implies that for a free throw taken in $s = \{3.1, 3.2, 3.3\}$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.25$ when compared to other contexts.

> This model implies that for a free throw taken in $s = \{3.4, 3.5\}$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.17$ when compared to other contexts.

> This model implies that for a free throw taken in $s = 4.1$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.22$ when compared to other contexts.

> This model implies that for a free throw taken in $s = 4.2$ we can expect in increase in the log-odds of $F$ being made by $\approx 0.16$ when compared to other contexts.

Given that some $s$ are certainly significant, it is worth it to keep $S_n$ as a predictor in our full model $m$. However, we have ruled out $D$ as being a predictor of $F$. Thus, though it is hard to say goodbye, we must do the needful if we wish to obtain an accurate $m$ which we can use in our function $f$.

This gives us a new updated table from our [Methodology] section:

```{=tex}
\begin{array}{c|ccccccc} 

& 1.1 & 1.2 & 1.3 & 1.4 & 1.5 & \cdots & 6.5 \\


H_D & m_{1.1} & m_{1.2} & m_{1.3} & m_{1.4} & m_{1.5} & \cdots & m_{6.5}\\ 

H_L & m_{1.1} & \ddots & m_{1.3} & \cdots & \cdots & \cdots & \vdots\\ 

T_G & m_{1.1} & n & \ddots & m_{1.4} & n & \cdots & \vdots\\ 

V_L & m_{1.1} & \cdots & \cdots & \ddots & m_{1.5} & \cdots & \vdots\\

V_D & m_{1.1} & \cdots & n & \cdots & \cdots & \cdots & m_{6.5}\\
\end{array}
```
Where we are no longer concerned about $D$.

Now we train and test our model at an 80% train, 20% test split using the same method shown earlier.

```{r ft-model-full, echo=TRUE}
##split data into training (80%) and testing (20%)
TRAIN_SPLIT = 0.8
TEST_SPLIT = 0.2
DATA_SIZE = nrow(ft_complete)

##reproduceable split
set.seed(715)
##choose between indexes 1, 2 with one index at 80% chance and the other at 20%
index_choice <- sample(2, DATA_SIZE, replace = TRUE, prob=c(TRAIN_SPLIT, TEST_SPLIT))
train_data <- ft_complete[index_choice==1, ]
test_data <- ft_complete[index_choice==2, ]

##train our model
ft_model_train <- glm(pts ~ stage_of_game + model_prob, data = train_data, family = "binomial")

##test our trained model
ft_model_test <- predict.glm(ft_model_train, test_data, type='response')

##print out the results of our tested predictions
head(ft_model_test, n = 10)

##add each model predicted free throw to the corresponding player
NUM_PLAYERS = nrow(test_data)
test_data_predictions <- test_data %>% mutate(model_prob_new = 0)
for (i in 1:NUM_PLAYERS) {
  test_data_predictions$model_prob_new[i] = ft_model_test[[i]]
}

##show only the player, n, m_p, and m
test_data_predictions <- test_data_predictions %>% ungroup() %>%
  select(stage_of_game, player, pts, ft_pct, model_prob, model_prob_new)

##print the first few lines of our predicted data
head(test_data_predictions, n = 10)
```

A lot has happened thus far, let us quickly recap.

First, our goal was to find, for free throw $F$, the best probability $P(F)$. We did this based on factoring in a player $p$'s statline, and created a model $m_p$ to use $p$'s statline to find a better $P(F)$ than the null model $n$ which is simply the `ft_pct` vector seen above. We did this in [Analysis: Finding $m_p$].

In this section, we analyzed what the crux of this report was about: the in-game context of $F$. We looked at how the stage of the game $S_n$ and the point differential $D$, influence $F$ and therefore $P(F)$. What we found was that $D$ was really inconsequential to $P(F)$. Only certain $S_n$ influenced $P(F)$. Painfully, we had to exclude $D$ and move on. This gave us an updated model $m$ that used the results of $m_p$ and $S_n$ as its predictors.

The output shown above numerically shows $P(F)$ across the models $n$ (`ft_pct`), $m_p$ (`model_prob`), and $m$ (`model_prob_new`). You can see that one noticeable difference is that each player has a different $P(F)$ under $m$ but shares the same $P(F)$ under $n$ and $m_p$ regardless of $S_n$.

#### Analysis: Finding $f$

Now, one item remains, creating $f$. Again, we stated that a parameter of $f$ is $D$. But now, this is not the case. $f$ only needs two parameters: $p$ and $s \in S_n$. Once these are passed in, we wish to give a table of all $P(F)$ across all models $\{n, m_p, m\}$ related to $p$ alongside relevant statistics of each model.

Recall that we made one important assumption of $F$.

$$
F \sim \text{Bernoulli(p)}
$$

This means that we can interpret $F$ as a Bernoulli random variable with parameter $p = P(F)$. Doing so gives us a great way to evaluate the efficiency of our models.

We simply randomly generate samples of $F$ (call it $F_r$). The number of samples of $F_r$  is equivalent to the count of $F$ in $s \in S_n$. $F_r$ will be parameterized by $P(F)$ given from our models. Then, we can simply compare the outcomes of all $F_r$ against the total outcomes of all $F$ as a measure of model accuracy. We will do this for all given $p$ and $S_n$ and return the results through our function $f$.


```{r function-setup, echo=TRUE}
##first, let's add all the P from our model 
## make predictions on all of the data
ft_model_probs <- predict.glm(ft_model_train, ft_complete, type='response')

##add each probability to the corresponding player
NUM_PLAYERS = nrow(ft_complete)
ft_complete <- ft_complete %>%
  mutate(model_prob_new = 0)
for(i in 1:NUM_PLAYERS) {
  ft_complete$model_prob_new[i] = ft_model_probs[[i]]
}

##show only the player, n, m_p, and m
ft_model_data <- ft_complete %>% ungroup() %>%
  select(stage_of_game, player, pts, ft_pct, model_prob, model_prob_new) %>%
  ##QoL
  rename(ft_result = pts,
         null_model = ft_pct, 
         player_model = model_prob, 
         full_model = model_prob_new) %>% 
    ##convert back to numeric to make f easy 
  mutate(stage_of_game = droplevels(stage_of_game))

##print the first 10 lines of this new data
head(ft_model_data, n = 10)
```

The output shown above shows $F$ (`ft_result`), and $P(F)$ across the models $n$ (`null_model`), $m_p$ (`player_model`), and $m$ (`full_model`).





One aspect of $f$ relies on us predicting $F$ via $F_r$: 

For $F_r$, we can take advantage of the following property of Bernoulli and Binomial Random Variables

Given random variables $X, Y$ where
$$
X \sim \text{Bernoulli(p)} \\ 
Y \sim \text{Binomial(n, p)}
$$
$Y$ can be obtained from $X$ by generating $n$ samples of $X$ and totaling those samples. That is: 

$$
Y = \sum_{i = 1}^n {(x_i \in X)}
$$
This is equivalent to saying: 
$$
\text{Binomial(n, p)} = \sum_{i = 1}^n \text{Bernoulli(p)}
$$


This result allows us to compare the number of successes of $F_r$ to the actual successes of $F$ since $F$ is our observed with $P(F)$ given by $n$ and $F_r$ is our predicted true $P(F)$ given by $\{m_p, m\}$. 

```{r function-building, echo=TRUE}
NUM_RVS = 1
NUM_SAMPLES = 1
get_ft_table <- function(player_name, stage_in_game) {
  ##get the requested data
  player_fts <- ft_model_data %>%
    filter(player == player_name & stage_of_game == stage_in_game)
  
  ##get the free throw data
  made_fts = sum(player_fts$ft_result)
  num_fts = nrow(player_fts)
  
  ##generate F_r for each free throw
  player_fts <- player_fts %>%
    mutate(F_null_bern = rbinom(NUM_RVS, NUM_SAMPLES, null_model)) %>%
    mutate(F_player_bern = rbinom(NUM_RVS, NUM_SAMPLES, player_model)) %>%
    mutate(F_full_bern = rbinom(NUM_RVS, NUM_SAMPLES, full_model))
  
  ##generate F_r for all free throws
  player_fts <- player_fts %>%
    mutate(F_null_ct = rbinom(NUM_RVS, num_fts, null_model)) %>%
    mutate(F_player_ct = rbinom(NUM_RVS, num_fts, player_model)) %>%
    mutate(F_full_ct = rbinom(NUM_RVS, num_fts, full_model))
  
  ## get the model probabilities
  null_model_prob = player_fts %>% pull(null_model) %>% mean()
  player_model_prob = player_fts %>% pull(player_model) %>% mean()
  full_model_prob = player_fts %>% pull(full_model) %>% mean()
  
  ## get the model prediction data: [F] success ct vs [Fr] success ct
  null_model_pred = mean(player_fts$F_null_ct) 
  player_model_pred = mean(player_fts$F_player_ct)
  full_model_pred = mean(player_fts$F_full_ct)
  
  ## get the model accuracy data: [F]  == [Fr]
  null_model_acc = 1 - (abs(made_fts - null_model_pred)/made_fts)
  player_model_acc = 1 - (abs(made_fts - player_model_pred)/made_fts)
  full_model_acc = 1 - (abs(made_fts - full_model_pred)/made_fts)
  
  ## prep work for the data frame 
  plyr <- c(player_name)
  models <- c("NULL MODEL", "PLAYER MODEL", "FULL MODEL")
  model_prob <- c(null_model_prob, player_model_prob, full_model_prob)
  model_accuracy <- c(null_model_acc, player_model_acc, full_model_acc)
  model_prediction <- c(null_model_pred, player_model_pred, full_model_pred)
  
  ## pool the columns into one data frame
  ft_shots_df <- data.frame(models, plyr, num_fts, made_fts, model_prob, model_prediction, model_accuracy)
  
  ## table formatting
  ft_shots_df <- ft_shots_df %>%  
    kable() %>%
     kable_styling(
        position = "center",
        full_width = FALSE,
        bootstrap_options = c("bordered", "condensed", "striped"),
     )
  
  ##return the data frame
  return(ft_shots_df)
}
```


Above we have built $f$. The output of $f$ consists of: 
   - `models` the model set ${n, m_p, m}$ 
   - `plyr` the player shooting the free throw
   - `num_fts` the number of free throw attempts by the `plyr` in this context
   - `made_fts` the number of made free throws by the `plyr` in this context
   - `model_prob` the probability given by the respective `models`
   - `model_prediction` the predicted made free throws by the respective `models`
   - `model_accuracy` the accuracy of `model_prediction` on `made_fts`

Now, we simply use $f$ to our heart's content.
```{r calling-f, echo =TRUE}
##get the table for Jayson Tatum during Sn = 1.2
get_ft_table("Jayson Tatum", 1.2)

##get the table for LeBron James during Sn = 2.5
get_ft_table("LeBron James", 2.4)

##get the table for Giannis Antetokounmpo during Sn = 3.3
get_ft_table("Giannis Antetokounmpo", 3.3)

##get the table for Jayson Tatum during Sn = 4.1
get_ft_table("Jayson Tatum", 4.1)

##get the table for Stephen Curry during Sn = 5.3
get_ft_table("Stephen Curry", 5.4)

##get the table for Grayson Allen during Sn = 6.5
get_ft_table("Grayson Allen", 6.5)
```

Before leaving the analysis, we wish to plot all $F$ against $S_n \times D$. It is a rather beautiful graph, but also gives quite a bit of insight on why we saw the results for $S_n$ and $D$ when crafting $m$

```{r all-F-graph, echo=TRUE}
ggplot(ft_complete, aes(x=stage_of_game, y=pts, color = pt_difference_cat)) + 
  geom_jitter(alpha = .5, height = .05) +
  geom_smooth(formula = "y ~ 1 + x", se=FALSE, method = "glm", method.args = list(family = "binomial")) +
  facet_wrap(vars(gm_quarter), scales = "free_x") + 
  theme_dark() +
  theme(plot.background = element_rect(fill = "papayawhip")) + 
  theme(legend.background = element_rect(fill = "papayawhip")) + 
  scale_colour_manual(
    values = c("papayawhip", "burlywood1", "darkgoldenrod1", 
               "cyan1", "cyan4")) +
  labs(color = "Point Differential") +
  xlab("Stage of the Game") +
  ylab("Free Throw Shot Result") +
  ggtitle("All Free Throw Shot Results By Quarter and Stage of Game", 
          subtitle = "2021-2022 NBA Regular Season")

```

Besides the lovely color palette, we can see how difficult a task it is to find any significance of $D$. It seems that no matter what the score is, $F$'s outcome is purely dependent on $p$. It is hard to even discern the right $S_n$ that may have a significant impact.

# Discussion

In this section, we will discuss the results of our tests and models and their implications in the context of our question and also their broader implications and limitations.

### Comparing the Graphs & Overall Conclusions

#### Graph Comparisons Discussion

Overall, given how much speculation was put on $S_n$ and $D$, it was surprising to see how insignificant both were. For one, $m$ chose not to include $D$ at all. For another, only a select few $S_n$ really had some impact on $P(F)$. Again, looking at the final graph that stored all $F$ across $S_n$ and $D$, we see just how random $F$ is if we only took those two into account.

### Shortcomings and Limitations

#### Limitation 1: $m_p$ and $m$

Recall that $m$ was built on $m_p$. This was probably not the smartest move on us. For one, $m_p$ already gave its own family of $P(F)$ and factored in the player only. When we created $m$, we used that $P(F)$ set as a regressor for $m$. In other words, if we were going to factor in $S_n$ and $D$, the best time to do it was when dealing with $m_p$, not $m$. One advantage this gave us was to allow the comparison of models $\{n, m_p, m\}$. But again, this came at a cost as $m$ tends to overestimate $P(F)$ severely. In the case of $p = \text{Zach LaVine}$, his $P(F) \approx 99\%$ when $S_n = 6.3$, a truly eye-opening result.

#### Limitation 2: $n$

If one were to run our $f$ a lot of times, more often than not, $n$ trumps $m_p$ and $m$. This result is not surprising given how the data was fetched to begin with. Recall that $n$ was simply the season average free throw percentage of $p$. That season average free throw percentage is based on the 2021-2022 NBA Season. The data? The data was queried from the 2021-2022 NBA season. $n$ has a clear advantage because it is a result of our data, so it will tend to have the best accuracy compared to our models. This is also a reason why our models $m_p$ and $m$ tend to overestimate $P(F)$ because of the positive effects of the $\beta \in m_p$ we observed when adding to $n$ and also similar positive effects of $S_n$ on $m_p$ we observed.

#### Limitation 3: $F$

An additional limitation was the oversimplification of $F$. Recall that $F$ was binary, and the Bernoulli random variable was chosen to model it. However, one important factor not really touched on about $F$ is this: $F$ generally, for all $p$, has a really high success rate. A quick calculation shows us that $F$ has a mean probability $P_\text{mean}(F) \approx 78\%$. That is an obnoxiously high number!

Typically, when evaluating a logistic regression model, there is some threshold $t \in [0, 1]$ that is chosen such that for some binary variable $X$

$$
X = 
\begin{cases} 
    1 & \text{if } P(X) \geq t \\
    0 & \text{otherwise} \\
\end{cases}
$$

However, it is hard to choose $t$ when $P_\text{mean}(F) \approx 78\%$. Thus, using $F$ as a Bernoulli, though easy to calculate accuracy, may not be the best method for evaluating $F$ and the efficiency of the model set $\{n, m_p, m\}$ and the probability set that results from it.

### Alternate Approaches

#### Alternate Approach 1: Use a different $n$

One obvious approach would be to fetch all $n$ from the 2020-2021 NBA Season and then use that to build $m_p$ and $m$. This would allow for all of the $\beta$ we chose in $m_p$ and the categorical $S_n$ and potentially $D$ to correct for the error inherent by using the prior season data as a reference point for the current season data.

#### Alternate Approach 2: Focus on $m$

Another alternate approach would be to avoid building $m_p$ altogether and just build some model $m$. This would not only reduce the expensive calculation costs, but it would also avoid the trivial case of adding more predictors. Using CV-techniques and Variable Selection techniques like Ridge Regression and LASSO and choosing a $\lambda$ to penalize overfitting as we encountered would also strengthen $m$ when compared to our lackluster model.

### Potential Future Directions

#### Future Direction 1

One future direction that a project like this can go is in predicting the NBA 3-point shot, 2-point shot, and overall field-goal percentage. The methods used above can be translated easily to such expeditions, albeit a better choice of variables is needed to do so.

#### Future Direction 2

Another future direction this project allows is for predicting home-team vs away-team advantages only. Use only this distinction as a categorical variable and keep all $p$ statline the same. Then, choose to find some $m$ for some binary outcome variable in the basketball game that may be impacted by this team-based distinction.

# References
